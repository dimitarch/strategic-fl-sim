# Twitter Federated Learning Configuration

experiment:
  id: "twitter_experiment"
  save_dir: "./results/twitter"

model:
  max_length: 512         # For Twitter tokenization

training:
  T: 50                      # Number of training rounds
  lr: 0.06                   # Learning rate
  local_steps: 1             # Number of local steps per round
  batch_size: 16          # For Twitter (smaller due to BERT memory)
  eval_batch_size: 32     # For Twitter

clients:
  n_players: 3               # Number of clients per round
  alpha_0: 1.0               # Scaling factor for good clients
  alpha_1: 1.0               # Scaling factor for bad clients
  beta_0: 0.0                # Noise level for good clients
  beta_1: 0.0                # Noise level for bad clients

aggregation:
  method: "mean"             # "weighted_average", "median", "trimmed_mean"

data:
  train_path: "./data/twitter/train.json"
  test_path: "./data/twitter/test.json"
  min_samples: 15            # Minimum samples per client
  max_samples: 20            # Maximum samples per client
