{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89dad4d",
   "metadata": {},
   "source": [
    "# StrategicFL: Twitter/Sent140 Demo Experiment\n",
    "\n",
    "Demonstrating strategic federated learning with adversarial clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06cfd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the base directory to Python path (go up one level from examples/)\n",
    "base_dir = os.path.abspath(\n",
    "    os.path.join(\n",
    "        os.path.dirname(__file__) if \"__file__\" in globals() else os.getcwd(), \"../..\"\n",
    "    )\n",
    ")\n",
    "if base_dir not in sys.path:\n",
    "    sys.path.insert(0, base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3eaebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from twitterdataset import TwitterDataset\n",
    "from models.bert import BertWithClassifier\n",
    "from strategicfl.agents import Client, Server\n",
    "from strategicfl.training.evaluate import evaluate_with_ids\n",
    "from strategicfl.training.metrics import get_gradient_metrics\n",
    "from strategicfl.utils.actions import create_scalar_action\n",
    "from strategicfl.utils.aggregation import get_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b964c24",
   "metadata": {},
   "source": [
    "## Preparation/initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c0acd",
   "metadata": {},
   "source": [
    "### Setup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377cf9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # GPU\n",
    "elif torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")  # Apple M-series\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # CPU\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3661fdc",
   "metadata": {},
   "source": [
    "### Setup experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74eaf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'training': {\n",
    "        'T': 10,\n",
    "        'lr': 0.06,\n",
    "        'local_steps': 1,\n",
    "        'batch_size': 16,\n",
    "        'eval_batch_size': 32\n",
    "    },\n",
    "    'clients': {\n",
    "        'n_players': 3,\n",
    "        'alpha_0': 1.0,\n",
    "        'alpha_1': 1.0,\n",
    "        'beta_0': 0.0,\n",
    "        'beta_1': 0.0\n",
    "    },\n",
    "    'aggregation': {\n",
    "        'method': 'mean'\n",
    "    },\n",
    "    'data': {\n",
    "        'train_path': '../../data/twitter/train.json',\n",
    "        'test_path': '../../data/twitter/test.json',\n",
    "        'min_samples': 15,\n",
    "        'max_samples': 20\n",
    "    },\n",
    "    'model': {\n",
    "        'max_length': 512\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for section, values in config.items():\n",
    "    print(f\"  {section}:\")\n",
    "    for key, value in values.items():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3a284a",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4246353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_bert_encoder(model):\n",
    "    \"\"\"Freeze BERT encoder layers, keep only classifier trainable.\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"classifier\" in name:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "def get_data(path: str):\n",
    "    \"\"\"Load data from JSON file.\"\"\"\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        user_names = data[\"users\"]\n",
    "        data_dict = data[\"user_data\"]\n",
    "    return data_dict, user_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4e7dcf",
   "metadata": {},
   "source": [
    "### Loading the Sent140 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d393b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLoading Twitter data...\")\n",
    "data_dict, user_names = get_data(config['data']['train_path'])\n",
    "test_data_dict, user_names_test = get_data(config['data']['test_path'])\n",
    "\n",
    "print(f\"Training data: {len(user_names)} users\")\n",
    "print(f\"Test data: {len(user_names_test)} users\")\n",
    "\n",
    "# Verify consistency between train and test user names\n",
    "usernames_diff = list(set(user_names) ^ set(user_names_test))\n",
    "assert not usernames_diff, \"Inconsistent usernames between test and train\"\n",
    "\n",
    "# DATA FILTERING\n",
    "if 'min_samples' in config['data'] and 'max_samples' in config['data']:\n",
    "    subset = [\n",
    "        user_name\n",
    "        for user_name in user_names\n",
    "        if config['data']['min_samples']\n",
    "        <= len(data_dict[user_name][\"y\"])\n",
    "        <= config['data']['max_samples']\n",
    "    ]\n",
    "    print(f\"Filtered to {len(subset)} users with {config['data']['min_samples']}-{config['data']['max_samples']} samples\")\n",
    "\n",
    "    user_names = subset\n",
    "    data_dict = {key: data_dict[key] for key in subset}\n",
    "    test_data_dict = {key: test_data_dict[key] for key in subset}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb7173",
   "metadata": {},
   "source": [
    "### Creating a Server agent with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8523cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating server...\")\n",
    "server_model = BertWithClassifier().to(device)\n",
    "\n",
    "# Freeze BERT encoder, keep only classifier trainable\n",
    "freeze_bert_encoder(server_model)\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in server_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in server_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.1f}%)\")\n",
    "\n",
    "server = Server(\n",
    "    device=device,\n",
    "    model=server_model,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.SGD(\n",
    "        [p for p in server_model.parameters() if p.requires_grad],\n",
    "        lr=config['training']['lr']\n",
    "    ),\n",
    "    aggregate_fn=get_aggregate(method=config['aggregation']['method']),\n",
    "    agent_id=\"server\",\n",
    ")\n",
    "\n",
    "print(f\"Created server with {config['aggregation']['method']} aggregation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532575d5",
   "metadata": {},
   "source": [
    "### Creating a group of Client agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating clients...\")\n",
    "split_index = len(user_names) // config['clients']['n_players']\n",
    "clients = []\n",
    "\n",
    "for i in range(config['clients']['n_players']):\n",
    "    start_idx = max(0, i * split_index)\n",
    "    end_idx = min((i + 1) * split_index, len(user_names))\n",
    "    client_user_names = user_names[start_idx:end_idx]\n",
    "\n",
    "    if i == config['clients']['n_players'] - 1:\n",
    "        alpha = config['clients']['alpha_1']\n",
    "        beta = config['clients']['beta_1']\n",
    "        agent_id = \"bad\"\n",
    "        print(f\"  Client {i+1}: ADVERSARIAL (α={alpha}, β={beta}) - {len(client_user_names)} users\")\n",
    "    else:\n",
    "        alpha = config['clients']['alpha_0']\n",
    "        beta = config['clients']['beta_0']\n",
    "        agent_id = f\"good{i}\"\n",
    "        print(f\"  Client {i+1}: honest (α={alpha}, β={beta}) - {len(client_user_names)} users\")\n",
    "\n",
    "    train_dataset = TwitterDataset(\n",
    "        client_user_names,\n",
    "        data_dict,\n",
    "        max_length=config['model']['max_length']\n",
    "    )\n",
    "    test_dataset = TwitterDataset(\n",
    "        client_user_names,\n",
    "        test_data_dict,\n",
    "        max_length=config['model']['max_length']\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        shuffle=True,\n",
    "        pin_memory=True if device.type == \"cuda\" else False,\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config['training']['eval_batch_size'],\n",
    "        shuffle=False,\n",
    "        pin_memory=True if device.type == \"cuda\" else False,\n",
    "    )\n",
    "\n",
    "    client_model = BertWithClassifier().to(device)\n",
    "    freeze_bert_encoder(client_model)\n",
    "\n",
    "    client = Client(\n",
    "        device=device,\n",
    "        train_dataloader=train_dataloader,\n",
    "        test_dataloader=test_dataloader,\n",
    "        model=client_model,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.SGD(\n",
    "            [p for p in client_model.parameters() if p.requires_grad],\n",
    "            lr=config['training']['lr']\n",
    "        ),\n",
    "        action=create_scalar_action(alpha, beta),\n",
    "        agent_id=agent_id,\n",
    "    )\n",
    "\n",
    "    clients.append(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befc24f7",
   "metadata": {},
   "source": [
    "## Run federated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2af6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting federated training for {config['training']['T']} rounds...\")\n",
    "\n",
    "all_losses, all_metrics = server.train(\n",
    "    clients=clients,\n",
    "    T=config[\"training\"][\"T\"],\n",
    "    get_metrics=get_gradient_metrics,\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd15ed0",
   "metadata": {},
   "source": [
    "## Evaluate final performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd8c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might be extremely slow if you're doing it locally, sorry :(\n",
    "\n",
    "print(\"\\nEvaluating final performance...\")\n",
    "final_accuracy, final_loss = evaluate_with_ids(server, clients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7db96",
   "metadata": {},
   "source": [
    "## Visualize training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04237085",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_array = np.array(\n",
    "    [[loss for loss in round_losses] for round_losses in all_losses]\n",
    ")\n",
    "grad_norms_array = np.array([metrics[\"grad_norms\"] for metrics in all_metrics])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Training losses over time\n",
    "axes[0, 0].plot(losses_array.mean(axis=1), label=\"Average Loss\", color=\"blue\")\n",
    "axes[0, 0].fill_between(\n",
    "    range(len(losses_array)),\n",
    "    losses_array.min(axis=1),\n",
    "    losses_array.max(axis=1),\n",
    "    alpha=0.3,\n",
    "    color=\"blue\",\n",
    ")\n",
    "axes[0, 0].set_title(\"Training Loss Over Time\")\n",
    "axes[0, 0].set_xlabel(\"Round\")\n",
    "axes[0, 0].set_ylabel(\"Loss\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Individual client losses\n",
    "for i in range(config[\"clients\"][\"n_players\"]):\n",
    "    client_type = (\n",
    "        \"Adversarial\" if i == config[\"clients\"][\"n_players\"] - 1 else f\"Honest {i + 1}\"\n",
    "    )\n",
    "    axes[0, 1].plot(losses_array[:, i], label=client_type, alpha=0.7)\n",
    "axes[0, 1].set_title(\"Individual Client Losses\")\n",
    "axes[0, 1].set_xlabel(\"Round\")\n",
    "axes[0, 1].set_ylabel(\"Loss\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Gradient norms\n",
    "for i in range(config[\"clients\"][\"n_players\"]):\n",
    "    client_type = (\n",
    "        \"Adversarial\" if i == config[\"clients\"][\"n_players\"] - 1 else f\"Honest {i + 1}\"\n",
    "    )\n",
    "    axes[1, 0].plot(grad_norms_array[:, i], label=client_type, alpha=0.7)\n",
    "axes[1, 0].set_title(\"Gradient Norms Over Time\")\n",
    "axes[1, 0].set_xlabel(\"Round\")\n",
    "axes[1, 0].set_ylabel(\"Gradient Norm\")\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Final loss bar plot\n",
    "client_ids = list(final_loss.keys())\n",
    "loss = [float(final_loss[client_id]) for client_id in client_ids]\n",
    "colors = [\n",
    "    \"red\"\n",
    "    if \"adversarial\" in client_id.lower() or \"bad\" in client_id.lower()\n",
    "    else \"green\"\n",
    "    for client_id in client_ids\n",
    "]\n",
    "axes[1, 1].bar(client_ids, loss, color=colors, alpha=0.7)\n",
    "axes[1, 1].set_title(\"Final Test Loss\")\n",
    "axes[1, 1].set_ylabel(\"Loss\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b50a7d",
   "metadata": {},
   "source": [
    "### Experiment summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nConfiguration:\")\n",
    "print(\n",
    "    f\"  Honest clients (a={config['clients']['alpha_0']}, b={config['clients']['beta_0']})\"\n",
    ")\n",
    "print(\n",
    "    f\"  Adversarial client (a={config['clients']['alpha_1']}, b={config['clients']['beta_1']})\"\n",
    ")\n",
    "print(f\"  Training rounds: {config['training']['T']}\")\n",
    "\n",
    "print(f\"\\nFinal Results with {config['aggregation']['method']} aggregation:\")\n",
    "for client_id, acc in final_accuracy.items():\n",
    "    loss = final_loss[client_id]\n",
    "    acc_val = float(acc) if hasattr(acc, \"item\") else acc\n",
    "    loss_val = float(loss) if hasattr(loss, \"item\") else loss\n",
    "    print(f\"  {client_id}: {acc_val:.4f} accuracy, {loss_val:.4f} loss\")\n",
    "\n",
    "# For the final comparison, get the adversarial client's performance\n",
    "adversarial_acc = final_accuracy.get(\"bad\", final_accuracy.get(\"adversarial\"))\n",
    "honest_accs = [\n",
    "    final_accuracy[k]\n",
    "    for k in final_accuracy.keys()\n",
    "    if k != \"bad\" and k != \"adversarial\"\n",
    "]\n",
    "\n",
    "if adversarial_acc is not None and honest_accs:\n",
    "    adversarial_val = (\n",
    "        float(adversarial_acc) if hasattr(adversarial_acc, \"item\") else adversarial_acc\n",
    "    )\n",
    "    honest_vals = [float(acc) if hasattr(acc, \"item\") else acc for acc in honest_accs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5925f855",
   "metadata": {},
   "source": [
    "### [Optional] Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64567f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"config\": config,\n",
    "    \"final_accuracy\": final_accuracy,\n",
    "    \"final_loss\": final_loss,\n",
    "    \"training_losses\": losses_array,\n",
    "    \"gradient_norms\": grad_norms_array,\n",
    "}\n",
    "\n",
    "with open(\"strategicfl_demo_shakespeare.pkl\", \"wb\") as f:\n",
    "    pickle.dump(results, f)\n",
    "print(\"Results saved to strategicfl_demo_shakespeare.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
